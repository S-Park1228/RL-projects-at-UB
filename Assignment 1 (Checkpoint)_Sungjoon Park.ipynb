{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f88a4d",
   "metadata": {},
   "source": [
    "# 1.1 Deterministic Environment\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91a6a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3adddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import widgets\n",
    "import time\n",
    "\n",
    "output_grid = widgets.Grid(1, 1)\n",
    "for _ in range(5):\n",
    "    grid = np.zeros((4, 4))\n",
    "    grid[np.random.randint(4), np.random.randint(4)] = 1\n",
    "    with output_grid.output_to(0, 0):\n",
    "        output_grid.clear_cell()\n",
    "        plt.imshow(grid)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ba792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnvironment(gym.Env):\n",
    "    metadata = { 'render.modes': [] }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.observation_space = spaces.Discrete(16)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.max_timesteps = 15\n",
    "        \n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.pos_1 = [0, 3]\n",
    "        self.pos_2 = [3, 0]\n",
    "        self.pos_3 = [1, 2]\n",
    "        self.goal_pos = [3, 3]\n",
    "        self.state = np.zeros((4, 4))\n",
    "        self.state[tuple(self.agent_pos)] = 1\n",
    "        self.state[tuple(self.pos_1)] = 0.2\n",
    "        self.state[tuple(self.pos_2)] = 0.2\n",
    "        self.state[tuple(self.pos_3)] = 0.5\n",
    "        self.state[tuple(self.goal_pos)] = 0.7\n",
    "        observation = self.state.flatten()\n",
    "        return observation\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0: # down\n",
    "            self.agent_pos[0] += 1\n",
    "        if action == 1: # up\n",
    "            self.agent_pos[0] -= 1\n",
    "        if action == 2: # right\n",
    "            self.agent_pos[1] += 1\n",
    "        if action == 3: # left\n",
    "            self.agent_pos[1] -= 1\n",
    "\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, 3)\n",
    "        self.state = np.zeros((4, 4))\n",
    "        self.state[tuple(self.agent_pos)] = 1\n",
    "        self.state[tuple(self.pos_1)] = 0.2\n",
    "        self.state[tuple(self.pos_2)] = 0.2\n",
    "        self.state[tuple(self.pos_3)] = 0.5\n",
    "        self.state[tuple(self.goal_pos)] = 0.7\n",
    "        observation = self.state.flatten()\n",
    "        \n",
    "        reward = 0\n",
    "        if (self.agent_pos == self.pos_1).all():\n",
    "            reward = -0.5\n",
    "        if (self.agent_pos == self.pos_2).all():\n",
    "            reward = -0.5\n",
    "        if (self.agent_pos == self.pos_3).all():\n",
    "            reward = 0.5\n",
    "        if (self.agent_pos == self.goal_pos).all():\n",
    "            reward = 1.0\n",
    "        \n",
    "        self.timestep += 1\n",
    "        if self.timestep >= self.max_timesteps:\n",
    "            done = True\n",
    "        else:\n",
    "            if (self.agent_pos == self.goal_pos).all():\n",
    "                done = True\n",
    "            else:\n",
    "                done = False\n",
    "        info.update({self.timestep: (np.clip(self.agent_pos, 0, 3), reward)})\n",
    "        \n",
    "        return observation, reward, done, info\n",
    "        \n",
    "    def render(self):\n",
    "        plt.imshow(self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a1db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "    def step(self, observation):\n",
    "        if counter == 0:\n",
    "            return np.random.choice(self.action_space.n)\n",
    "        else:\n",
    "            loc = tuple(list(info.values())[-1])\n",
    "            if tuple(loc[0])[0] >= 3:\n",
    "                return 2\n",
    "            else:\n",
    "                if tuple(loc[0])[1] >= 3:\n",
    "                    return 0\n",
    "                else:\n",
    "                    if np.random.random() > 0.5:\n",
    "                        return 2\n",
    "                    else:\n",
    "                        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3955ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridEnvironment()\n",
    "agent = RandomAgent(env)\n",
    "info = {}\n",
    "counter = 0\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "output_grid = widgets.Grid(1, 1)\n",
    "with output_grid.output_to(0, 0):\n",
    "    env.render()\n",
    "\n",
    "\n",
    "while not done:\n",
    "    action = agent.step(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    counter += 1\n",
    "    with output_grid.output_to(0, 0):\n",
    "        output_grid.clear_cell()\n",
    "        env.render()\n",
    "    time.sleep(1)\n",
    "\n",
    "temp = 0\n",
    "for i in range(len(info)):\n",
    "    print(f'In the step {tuple(list(info.keys())[i])}, the location is {list(info.values())[i][0]} and the reward is {list(info.values())[i][1]}.')\n",
    "    temp += list(info.values())[i][1]\n",
    "print(f'The total reward is {temp}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd563f8",
   "metadata": {},
   "source": [
    "# 1.2 Stochastic Environment\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35705bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnvironment_Stochastic(gym.Env):\n",
    "    metadata = { 'render.modes': [] }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.observation_space = spaces.Discrete(16)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.max_timesteps = 15\n",
    "        \n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.pos_1 = [0, 3]\n",
    "        self.pos_2 = [3, 0]\n",
    "        self.pos_3 = [1, 2]\n",
    "        self.goal_pos = [3, 3]\n",
    "        self.state = np.zeros((4, 4))\n",
    "        self.state[tuple(self.agent_pos)] = 1\n",
    "        self.state[tuple(self.pos_1)] = 0.2\n",
    "        self.state[tuple(self.pos_2)] = 0.2\n",
    "        self.state[tuple(self.pos_3)] = 0.5\n",
    "        self.state[tuple(self.goal_pos)] = 0.7\n",
    "        observation = self.state.flatten()\n",
    "        return observation\n",
    "    \n",
    "    def step(self, action):\n",
    "        p = np.random.random()\n",
    "        if action == 0: # down\n",
    "            if p < 0.85:\n",
    "                self.agent_pos[0] += 1\n",
    "            elif p < 0.9:\n",
    "                self.agent_pos[0] -= 1\n",
    "            elif p < 0.95:\n",
    "                self.agent_pos[1] += 1\n",
    "            else:\n",
    "                self.agent_pos[1] -= 1\n",
    "        if action == 1: # up\n",
    "            if p < 0.85:\n",
    "                self.agent_pos[0] -= 1\n",
    "            elif p < 0.9:\n",
    "                self.agent_pos[0] += 1\n",
    "            elif p < 0.95:\n",
    "                self.agent_pos[1] += 1\n",
    "            else:\n",
    "                self.agent_pos[1] -= 1\n",
    "        if action == 2: # right\n",
    "            if p < 0.85:\n",
    "                self.agent_pos[1] += 1\n",
    "            elif p < 0.9:\n",
    "                self.agent_pos[0] += 1\n",
    "            elif p < 0.95:\n",
    "                self.agent_pos[0] -= 1\n",
    "            else:\n",
    "                self.agent_pos[1] -= 1\n",
    "        if action == 3: # left\n",
    "            if p < 0.85:\n",
    "                self.agent_pos[1] -= 1\n",
    "            elif p < 0.9:\n",
    "                self.agent_pos[0] == 1\n",
    "            elif p < 0.95:\n",
    "                self.agent_pos[0] -= 1\n",
    "            else:\n",
    "                self.agent_pos[1] += 1\n",
    "\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, 3)\n",
    "        self.state = np.zeros((4, 4))\n",
    "        self.state[tuple(self.agent_pos)] = 1\n",
    "        self.state[tuple(self.pos_1)] = 0.2\n",
    "        self.state[tuple(self.pos_2)] = 0.2\n",
    "        self.state[tuple(self.pos_3)] = 0.5\n",
    "        self.state[tuple(self.goal_pos)] = 0.7\n",
    "        observation = self.state.flatten()\n",
    "        \n",
    "        reward = 0\n",
    "        if (self.agent_pos == self.pos_1).all():\n",
    "            reward = -0.5\n",
    "        if (self.agent_pos == self.pos_2).all():\n",
    "            reward = -0.5\n",
    "        if (self.agent_pos == self.pos_3).all():\n",
    "            reward = 0.5\n",
    "        if (self.agent_pos == self.goal_pos).all():\n",
    "            reward = 1.0\n",
    "        \n",
    "        self.timestep += 1\n",
    "        if self.timestep >= self.max_timesteps:\n",
    "            done = True\n",
    "        else:\n",
    "            if (self.agent_pos == self.goal_pos).all():\n",
    "                done = True\n",
    "            else:\n",
    "                done = False\n",
    "        info.update({self.timestep: (np.clip(self.agent_pos, 0, 3), reward)})\n",
    "        \n",
    "        return observation, reward, done, info\n",
    "        \n",
    "    def render(self):\n",
    "        plt.imshow(self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3166eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridEnvironment_Stochastic()\n",
    "agent = RandomAgent(env)\n",
    "info = {}\n",
    "counter = 0\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "output_grid = widgets.Grid(1, 1)\n",
    "with output_grid.output_to(0, 0):\n",
    "    env.render()\n",
    "\n",
    "\n",
    "while not done:\n",
    "    action = agent.step(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    counter += 1\n",
    "    with output_grid.output_to(0, 0):\n",
    "        output_grid.clear_cell()\n",
    "        env.render()\n",
    "    time.sleep(1)\n",
    "\n",
    "temp = 0\n",
    "for i in range(len(info)):\n",
    "    print(f'In the step {list(info.keys())[i]}, the location is {tuple(list(info.values())[i][0])} and the reward is {list(info.values())[i][1]}.')\n",
    "    temp += list(info.values())[i][1]\n",
    "print(f'The total reward is {temp}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c429500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
